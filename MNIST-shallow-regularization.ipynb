{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1794a4e",
   "metadata": {},
   "source": [
    "# Finding the perfect model\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nhartman94/TTT-model-building/blob/main/MNIST-shallow-regularization.ipynb)\n",
    "\n",
    "**Goal:** We learned in the lecture a set of tricks for how to build a model that's \"just right\" for the data!\n",
    "\n",
    "In this tutorial, we're going to learn how use these training and model building tricks to train a model in pytorch for a toy dataset.\n",
    "\n",
    "We'll focus on understanding what's happening under the hood to gain intuition for what these \"Occom's razor\" regularization tricks are doing.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. Data visualization \n",
    "\n",
    "- **Q1:** Plot the avg images\n",
    "\n",
    "2. Model setup\n",
    "\n",
    "- **Q2:** What's the loss before training (analystical calc)\n",
    "- **Q3:** What's the loss before training (code check)\n",
    "\n",
    "3. First training\n",
    "\n",
    "- **Q4:** Compare with the validation loss\n",
    "\n",
    "4. Regularization techniques\n",
    "    - 4a) Batch normalization\n",
    "\n",
    "        - **Q5:** Visualize the output from the activations in the last step\n",
    "        - **Q6:** Implement batch norm in your model\n",
    "        - **Q7:** Compare the activations from the batch normalized model\n",
    "\n",
    "    - 4b) Dropout\n",
    "        - **Q8:** Implement dropout in the model\n",
    "\n",
    "    - 4c) (Bonus) L2 norm\n",
    "        - **Q9:** Visualize the weights for the models trained earlier\n",
    "        - **Q10:** Implement the L2 loss\n",
    "\n",
    "5. Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09dbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61446620",
   "metadata": {},
   "source": [
    "## 1. Data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3bc2f6",
   "metadata": {},
   "source": [
    "We're going to use the MNIST dataset as yesterday in Israt's tutorial.\n",
    "\n",
    "(Caveat: here testing out a smaller one, 8x8 images instead of 28x28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da497c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce177de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits['data']\n",
    "y = digits[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1753fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / val / test (80/20/20) split\n",
    "\n",
    "# 20% test data\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(\n",
    "    torch.FloatTensor(X), torch.LongTensor(y), test_size=0.2\n",
    ")\n",
    "\n",
    "# split validation set, \n",
    "# 25% of remaining data goes into the validation set\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = \\\n",
    "    train_test_split(X_tr, y_tr, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca515d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cea471",
   "metadata": {},
   "source": [
    "**Q1** Plot the avg image for each of the classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a5e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5,figsize=(16,7))\n",
    "\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "    # mean img\n",
    "    xi =  ... # your code here \n",
    "    ax.imshow(xi,cmap='Greys')\n",
    "    ax.set_title(f'y={i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This function will draw samples from the training batch\n",
    "'''\n",
    "\n",
    "N_tr = len(y_tr)\n",
    "print('# of training evts',N_tr)\n",
    "\n",
    "def get_train_data(bs=128):\n",
    "\n",
    "    idx = np.random.choice(N_tr, 128)\n",
    "\n",
    "    return X_tr[idx], y_tr[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff8597",
   "metadata": {},
   "source": [
    "## 2. Model setup\n",
    "\n",
    "We're at the ERUM deep learning course... so ofc we want to train a NN for classification üòÉ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1616ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: count the # of parameters\n",
    "def count_parameters(model):\n",
    "    return sum([sum(p.view(-1).shape) for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ccddde",
   "metadata": {},
   "source": [
    "Below we remind you how to build a model with a single hidden layer and 256 hidden units:\n",
    "\n",
    "üí° powers of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 64\n",
    "out_dim = 10\n",
    "\n",
    "# Starter code\n",
    "class myNN(nn.Module):\n",
    "    \"\"\"\n",
    "    We'll keep adding functionality to this NN as we go thru the\n",
    "    next exercises, but this is the starting skeleton\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, H=64):\n",
    "        super(myNN, self).__init__()\n",
    "\n",
    "        # In the init class, need to set the weights of the trainable weights\n",
    "        self.in_layer = nn.Linear(in_dim, H)\n",
    "        self.hid_layer = nn.Linear(H, H)\n",
    "        self.out_layer = nn.Linear(H, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # First linear transformation\n",
    "        x = self.in_layer(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        x = self.hid_layer(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        x = self.out_layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0623084",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = myNN(64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = m(X_tr)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353129e",
   "metadata": {},
   "source": [
    "### Loss function: Categorical cross entropy\n",
    "\n",
    "For targets $y=[0,1, ..., 9]$, our model is outputting $z \\in \\mathbb{R}^10$, the logits (unnormalized probabilities) for these 10 classes.\n",
    "\n",
    "We want to interpret the output of the model probabilistically, which we can do via the softmax:\n",
    "\n",
    "$$\\mathrm{Softmax}(z) \\rightarrow p_i = \\frac{\\exp(z_i)}{\\sum_{i=1}^K\\exp(z_i)}$$\n",
    "\n",
    "The **cross entropy** loss function is then the negative log likelihood of the training data .\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_i \\log p_{y_i},\n",
    "$$\n",
    "\n",
    "where $p_{y_i}$ is the predicted probability of the true target class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c586a",
   "metadata": {},
   "source": [
    "**Q2:** What's the loss of this randomly initialized network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm-up: plot the Softmax prob for the network `m`\n",
    "# (hint: use nn.Softmax() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04e3c5",
   "metadata": {},
   "source": [
    "**Q3:** üíª Check your calculation, what is the loss for your  model?\n",
    "\n",
    "**Tip:** You can either code up the loss fct yourself or use `F.cross_entropy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q3: YOUR CODE HERE\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c1720",
   "metadata": {},
   "source": [
    "## 3. First training\n",
    "\n",
    "OK... can we improve this loss by training? ‚öôÔ∏è‚öôÔ∏è\n",
    "\n",
    "Since you trained NNs in the tutorial yday we'll give you some starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a97d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soln to Q10 \n",
    "def get_L2_loss(model):\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47de36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code\n",
    "def train_model(model, lr=1e-3):\n",
    "\n",
    "    print(f\"training model with {count_parameters(model)} parameters\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    for i in range(1000):  # 1k training steps\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        xi, yi = get_train_data(128)  # Draw 128 samples\n",
    "        logits = model(xi)\n",
    "        loss = nn.CrossEntropyLoss()(logits, yi)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        train_losses.append(float(loss))\n",
    "\n",
    "        # Q4: Calc loss on validation set\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print(float(loss))\n",
    "    return (model, train_losses, val_losses) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5345d0c",
   "metadata": {},
   "source": [
    "Train the model and plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2444d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, tr_loss, val_loss = train_model(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ae2d5",
   "metadata": {},
   "source": [
    "**Q4:** What about the validation loss?\n",
    "\n",
    "- [ ] Add the functionality to `train_model`\n",
    "- [ ] Draw the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.eval()\n",
    "with torch.no_grad():\n",
    "    print(f\"Val loss: {F.cross_entropy(m(X_val), y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_loss,color='C0',label='train')\n",
    "plt.plot(val_loss,color='C0',label='val',ls='--')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af8408",
   "metadata": {},
   "source": [
    "**Q4b:** What do you think? Are we overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27daeb9a",
   "metadata": {},
   "source": [
    "## 4. Regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc74c3",
   "metadata": {},
   "source": [
    "### 4a) Batch normalization \n",
    "\n",
    "**Q5:** Plot the activations of the hidden units right before the ReLU\n",
    "\n",
    "(Fun fact, should be 64 b/c this is the size of the hidden latent space!)\n",
    "\n",
    "**Hint:** For this it will be a lot easier to use the functional API than the sequential one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f24caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q5: Plot the activations (on the validation set)\n",
    "\n",
    "(For inspiration, we show you the first layer)\n",
    "\n",
    "\"\"\"\n",
    "fig,axs = plt.subplots(1,3,figsize=(10,3))\n",
    "\n",
    "# Step 1: apply the input xform\n",
    "with torch.no_grad():\n",
    "    x = m.in_layer(X_val)  \n",
    "    \n",
    "axs[0].hist(x, 100, histtype=\"step\", alpha=0.5)\n",
    "\n",
    "# Step 2: apply the hidden activations\n",
    "\n",
    "... #<-  your code here\n",
    "\n",
    "# Step 3: Last output transform\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_title(f'Layer {i+1} activations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773da26",
   "metadata": {},
   "source": [
    "**What's your take away?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de91717",
   "metadata": {},
   "source": [
    "**Q6:** Add batch norm to the model archicture. \n",
    "- Tip: `nn.BatchNorm1d`\n",
    "- Put it right before the ReLU nonlinearity.\n",
    "\n",
    "Train the new model.\n",
    "\n",
    "How do the training and validation losses compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_bn = ... # <- your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd6af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4276ea41",
   "metadata": {},
   "source": [
    "**What do you think?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e105f4",
   "metadata": {},
   "source": [
    "**Q7:** Now plot the activations of the model trained with batch norm (a.k.a, plot the model output right after the batch norm layer).\n",
    "\n",
    "Are they closer to 0 mean, unit variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa41614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q7: Revise Q5, but w/ model trained w/ bn\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8a63c",
   "metadata": {},
   "source": [
    "**What do you think?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b4f34",
   "metadata": {},
   "source": [
    "### 4b) Dropout\n",
    "\n",
    "**Q8:** Add dropout to the model and compare trainings.\n",
    "\n",
    "Where to put it? put it after the ReLU nonlinearities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce89143",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q8: Your code here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555367d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c47dd9a",
   "metadata": {},
   "source": [
    "**What do you think?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c24b5",
   "metadata": {},
   "source": [
    "### 4c) (Bonus) L2 normalization\n",
    "\n",
    "**Q9:** Visualize the weights for our trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: To get the weight and bias of the first layer..\n",
    "print('W',m.in_layer.weight)\n",
    "print(\"b\", m.in_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e258f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "# input layer\n",
    "for i, ax in zip(range(3),axs):\n",
    "\n",
    "\n",
    "    ax.set_title(f\"Linear{i+1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2f3d9",
   "metadata": {},
   "source": [
    "**What do you think?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79986185",
   "metadata": {},
   "source": [
    "**Q10:** Implement the L2 loss and compare...\n",
    "\n",
    "Question for you... _where_ and how will you implement the L2 loss?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f813d",
   "metadata": {},
   "source": [
    "**Compare with the model weights after training w/ L2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023ba021",
   "metadata": {},
   "source": [
    "## 5. Evaluate on the test set\n",
    "\n",
    "Now that you're done w/ the optimizations on the val set... how did we do on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2488a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in after you're done w/ any final optimizations you want to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68720b4f",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "\n",
    "Great job! We've been diving into some of the guts of NNs and classic training techniques.\n",
    "\n",
    "Something you might have notices is your experiments are a bit noisy, e.g, rerunning w/ the a new random seed can produce different results.\n",
    "\n",
    "For a more robust study, we'd repeat each experiment 5-10 times and report the mean and error bar (the \"deep ensembles error\" that we discussed in lecture)... but the point here was just to get some hands-on-keyboard understanding of the concepts we were covering.\n",
    "\n",
    "Until next time... happy training üåû üöä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda03310",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
